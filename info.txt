docker stop qdrant
docker rm qdrant
docker run -d --name qdrant -p 6333:6333 qdrant/qdrant:latest
python -m app.api.gemma_api 2>&1 | tee gemma_api.log
ssh esel@10.42.0.12

uvicorn app.main:app --host 0.0.0.0 --port 8080

eval "$(ssh-agent -s)"
ssh-add ~/.ssh/mckhanster
ssh -T git@github.com

curl https://api.x.ai/v1/chat/completions \
-H "Content-Type: application/json" \
-H "Authorization: Bearer " \
-d '{
  "messages": [
    {
      "role": "system",
      "content": "You are Grok, a chatbot inspired by the Hitchhikers Guide to the Galaxy."
    },
    {
      "role": "user",
      "content": "Testing?"
    }
  ],
  "model": "grok-2-1212",
  "stream": false,
  "temperature": 0
}'

update skill 59, 'Make the network scanning script faster by scanning only common ports (22, 23, 80, 443, 445, 3389), using asynchronous scanning with real-time output, and handling interruptions gracefully'

sudo netstat -nlp | grep :8080
kill -9 $(lsof -t -i:8080)

You are generating Python code for a skill in an adaptive learning system for an agent name {agent_name} with the role {agent_role}, accessible via `memory_manager.get_context()` (returns e.g., "Agent Role: The Adaptive Skill Master and Continuous Learning Facilitator"). The skill is: `{skill_name}` (description: `{skill_description}`). The code must:
1. Implement the skillâ€™s core functionality (e.g., for `Skill Assessment`, analyze intent frequency to identify gaps).
2. Integrate with `memory_manager` (access `memory.db` via `memory_manager.get_interaction`, `get_similar_interactions`, or SQL queries to `interactions.intent`).
3. Use `sklearn` or `pandas` for ML or data analysis (e.g., clustering intents, predicting skills).
4. Accept 1-3 parameters via `sys.argv`, parsed as JSON (e.g., `sys.argv[1] = '[\"data\"]'` for lists, `{\"key\": \"value\"}` for objects).
5. Validate parameters, raising `ValueError` for invalid/missing inputs.
6. Return JSON output (e.g., `{"result": "value"}`) for CLI integration.
7. Avoid trivial code (no `print`-only logic); implement at least one function with ML or data-driven logic.
8. Limit to 100-200 lines, with clear comments and robust error handling.
9. Use `memory_manager.get_context()` to adapt to the current role dynamically (e.g., prioritize intents matching role keywords).

Return JSON:
{
  "code": "Python code with imports, validation, and skill logic",
  "parameters": [
    {
      "name": "param_name",
      "type": "str|int|float|bool|list[str]|list[int]|list[float]|list[bool]|object[class_name]",
      "description": "Brief description (20-50 words, e.g., 'User intent history from memory.db')",
      "required": bool,
      "default": "value or null for primitives, [] for lists, null for objects",
      "fields": [
        {"name": "field_name", "type": "str|int|float|bool", "description": "Field description (e.g., 'Intent label')"}
      ]
    }
  ]
}